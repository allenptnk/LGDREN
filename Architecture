import torch
import torch.nn as nn

class LGDREN(nn.Module):
    def __init__(self, num_res_blocks=32, num_feats=64, scale=2):
        super(LGDREN, self).__init__()

        # Initial convolution layer
        self.head = nn.Conv2d(3, num_feats, kernel_size=3, padding=1)

        # Channel Attention Restoration BlocK (CARB)
        self.body = nn.Sequential(
            *[ResidualChannelAttentionBlock(num_feats) for _ in range(num_res_blocks)]
        )

        # Frequency Based Enhancement Block (FBEB)
        self.feature_enhance = FrequencyFeatureEnhancement(num_feats)

        # Global skip connection (adjusted to match processed features)
        self.global_skip = nn.Conv2d(3, num_feats, kernel_size=3, padding=1)

        # Tail convolution layer for upscaling

        self.tail = nn.Sequential(
            nn.Conv2d(num_feats, num_feats * (scale ** 2), kernel_size=3, padding=1),
            nn.PixelShuffle(scale),
            nn.Conv2d(num_feats, 3, kernel_size=3, padding=1)  # Ensure final output has 3 channels (RGB)
        )


    def forward(self, x):
        # Save the input for global skip connection
        global_skip = self.global_skip(x)  # Align input channels to `num_feats`

        # Main processing pipeline
        x = self.head(x)
        x = self.body(x)
        x = self.feature_enhance(x)

        # Add global skip connection before upscaling
        x = x + global_skip  # Ensure both tensors have the same number of channels
        x = self.tail(x)

        return x
